# specify here default training configuration
defaults:
  - _self_

project: JGLUE-benchmark

# path to original working directory
# hydra hijacks working directory by changing it to the current log directory,
# so it's useful to have this path as a special variable
# https://hydra.cc/docs/next/tutorials/basic/running_your_app/working_directory
work_dir: ${hydra:runtime.cwd}

# seed for random number generators in pytorch, numpy and python.random
# "null" means the seed is randomly selected at runtime.
seed: null

# name of the run is accessed by loggers
# should be used along with experiment mode
name: ${hydra:job.config_name}-${hydra:job.override_dirname}

exp_dir: ${work_dir}/result/${name}
run_id: ${now:%m%d}_${now:%H%M%S}
run_dir: ${exp_dir}/${run_id}
config_name: ${hydra:job.config_name}

# compile model for faster training with pytorch 2.0
compile: false

hydra:
  run:
    dir: ${exp_dir}
  sweep:
    dir: ${work_dir}/multirun_result
    subdir: ${name}-${hydra:job.num}
  job:
    env_set:
      TOKENIZERS_PARALLELISM: false
    config:
      override_dirname:
        kv_sep: '_'
        item_sep: '-'
        exclude_keys:
          - seed
          - name
          - exp_dir
          - run_dir
          - devices
          - num_workers
          - checkpoint_path
          - logger
          - max_batches_per_device
